\documentclass[a4paper,10pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fourier}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cprotect}

\usepackage{tgtermes}

\usepackage[
pdftitle={Statistical Machine Learning}, 
pdfauthor={Inez Wijnands \& Guido Zuidhof, Radboud University Nijmegen},
colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue,bookmarks=true,
bookmarksopenlevel=2]{hyperref}
\usepackage{amsmath,amssymb,amsthm,textcomp}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{tikz}

\usepackage{geometry}
\geometry{total={210mm,297mm},
left=25mm,right=25mm,%
bindingoffset=0mm, top=20mm,bottom=20mm}

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1 instead of 1)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1 i/o 1)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1 i/of 1)

\linespread{1.35}

\newcommand{\linia}{\rule{\linewidth}{0.5pt}}

% custom theorems if needed
\newtheoremstyle{mytheor}
    {1ex}{1ex}{\normalfont}{0pt}{\scshape}{.}{1ex}
    {{\thmname{#1 }}{\thmnumber{#2}}{\thmnote{ (#3)}}}

\theoremstyle{mytheor}
\newtheorem{defi}{Definition}

% my own titles
\makeatletter
\renewcommand{\maketitle}{
\begin{center}
\vspace{2ex}
{\huge \textsc{\@title}}
\vspace{1ex}
\\
\linia\\
\@author  \@date
\vspace{4ex}
\end{center}
}
\makeatother
%%%

% custom footers and headers
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{Assignment \textnumero{} 3}
\cfoot{}
\rfoot{Page \thepage\ /\ \pageref*{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
%

% code listing settings
\usepackage{listings}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    aboveskip={0.9\baselineskip},
    belowskip={0.9\baselineskip},
    columns=fixed,
    extendedchars=true,
    breaklines=true,
    tabsize=4,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=lines,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\color[rgb]{0.1,0.126,0.941},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color[rgb]{0,0.5,0},
    numbers=left,
    numberstyle=\scriptsize\ttfamily,
    stepnumber=1,
    numbersep=10pt,
    captionpos=t,
    escapeinside={\%*}{*)}
}

%%%----------%%%----------%%%----------%%%----------%%%

\begin{document}

\title{Statistical Machine Learning \\ Assignment 3}

\author{Inez Wijnands (s4149696) \& Guido Zuidhof (s4160703)\\ Radboud University Nijmegen\\}

\date{03/12/2015}

\maketitle

\noindent \textit{The entire code listing is included in the zip-file. The listings shown here are merely code snippets}.\vspace{-0.5cm}
\section{Bayesian linear regression}
\begin{enumerate}
	\item \begin{align}
		\{x_1,t_1\} &= (0.4, 0.05)\\
		\{x_2,t_2\} &= (0.6, -0.35)\\
		\alpha &= 2\\
		\beta &= 10\\
		\end{align}
		\noindent We need to compute the predictive distribution $p(t | x,\boldsymbol{t},\boldsymbol{x}, \alpha, \beta)$ after the two data points (where $x$ is the input and $t$ is the target) are observed.\\\\
		\textbf{Step 1:} Identify the vector of basis functions $\phi(\boldsymbol{x})$, and write out $\boldsymbol{\Phi}^T\boldsymbol{t}$ and $\boldsymbol{\Phi}^T\boldsymbol{\Phi}$ in terms of the data $\{x_n,t_n\}$.
		\begin{align}
		\boldsymbol{\Phi}^T\boldsymbol{t} &= \sum_n \boldsymbol{\phi}(x_n)t_n = N\begin{pmatrix} \bar{\mu}_t \\ \bar{\mu}_{xt}\end{pmatrix}\\
		\boldsymbol{\Phi}^T\boldsymbol{\Phi} &= \sum_n \boldsymbol{\phi}(x_n)\boldsymbol{\phi}(x_n)^T = N\begin{pmatrix} 1 & \bar{\mu}_x \\ \bar{\mu}_x & \bar{\mu}_{xx} \end{pmatrix}
		\end{align}
		Where: 
		\begin{align*}
		\bar{\mu}_t = \frac{1}{N} \sum_n t_n  & &  \bar{\mu}_{xt} = \frac{1}{N} \sum_n x_nt_n \\
		\bar{\mu}_x = \frac{1}{N} \sum_n x_n & & \bar{\mu}_{xx} = \frac{1}{N} \sum_n x_n^2
		\end{align*}
		Combined:
		\begin{align}
		\boldsymbol{\Phi}^T\boldsymbol{t} &= \sum_n \boldsymbol{\phi}(x_n)t_n = N\begin{pmatrix} \bar{\mu}_t \\ \bar{\mu}_{xt}\end{pmatrix}\\
		& = N\begin{pmatrix} \frac{1}{N} \sum_n t_n \\ \frac{1}{N} \sum_n x_nt_n\end{pmatrix} \\
		& = 2\begin{pmatrix} \frac{1}{2} (0.05 + -0.35) \\ \frac{1}{2} (0.4\cdot 0.05 + 0.6 \cdot -0.35)\end{pmatrix} \\
		& = \begin{pmatrix} -0.3 \\  -0.19 \end{pmatrix}
		\end{align}
		\begin{align}
		\boldsymbol{\Phi}^T\boldsymbol{\Phi} &= \sum_n \phi(x_n)t_n = N\begin{pmatrix} 1 & \bar{\mu}_x \\ \bar{\mu}_x & \bar{\mu}_{xx} \end{pmatrix}\\
		& = N\begin{pmatrix} 1 & \frac{1}{N} \sum_n x_n \\ \frac{1}{N} \sum_n x_n & \frac{1}{N} \sum_n x_n^2 \end{pmatrix}\\
		& = 2\begin{pmatrix} 1 & \frac{1}{2} (0.4 + 0.6) \\ \frac{1}{2} (0.4 + 0.6) & \frac{1}{2} (0.4^2 + 0.6^2) \end{pmatrix}\\
		& = \begin{pmatrix} 2 & 1 \\ 1 & 0.52 \end{pmatrix}
		\end{align}\\
		\textbf{Step 2:} Compute the posterior $p(\boldsymbol{w} | \boldsymbol{t}, \boldsymbol{x}, \alpha, \beta)$. 
		\begin{align}
		p(\boldsymbol{w} | \alpha) &= \mathcal{N}(\boldsymbol{w} | 0, \alpha^{-1}\boldsymbol{I})\\
		p(\boldsymbol{t} | \boldsymbol{w}, \boldsymbol{x}) &= \mathcal{N}(\boldsymbol{t} | \boldsymbol{\Phi}\boldsymbol{w}, \beta^{-1}\boldsymbol{I})\\
		&\rightarrow\\
		p(\boldsymbol{w} | \boldsymbol{t}, \boldsymbol{x}) &= \mathcal{N}(\boldsymbol{w} | \boldsymbol{m}_N, \boldsymbol{S}_N)\\	
		\boldsymbol{S}_N^{-1} &= \alpha \boldsymbol{I} + \beta \boldsymbol{\Phi}^T\boldsymbol{\Phi} = \begin{pmatrix} \alpha & 0 \\ 0 & \alpha \end{pmatrix} + N\beta \begin{pmatrix} 1 & \bar{\mu}_x \\ \bar{\mu}_x & \bar{\mu}_{xx} \end{pmatrix}\\
		&= \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} + 10 \begin{pmatrix} 2 & 1 \\ 1 & 0.52 \end{pmatrix}\\
		&=  \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} + \begin{pmatrix} 20 & 10 \\ 10 & 5.2 \end{pmatrix} \\
		&=  \begin{pmatrix} 22 & 10 \\ 10 & 7.2 \end{pmatrix}\\
		\boldsymbol{m}_N &= \beta \boldsymbol{S}_N \boldsymbol{\Phi}^T\boldsymbol{t} = N\beta \boldsymbol{S}_N \begin{pmatrix} \bar{\mu}_t \\ \bar{\mu}_{xt} \end{pmatrix}\\
		&= 10 \begin{pmatrix} 22 & 10 \\ 10 & 7.2 \end{pmatrix}^{-1} \begin{pmatrix} -0.3 \\  -0.19 \end{pmatrix} \\
		&= \begin{pmatrix} -0.0445 \\  -0.2021 \end{pmatrix}
		\end{align}\\
		\textbf{Step 3:} Compute the predictive distribution $p(t | x,\boldsymbol{t},\boldsymbol{x}, \alpha, \beta) = \mathcal{N}(t | m(x), s^2(x))$ in terms of known or computable quantities, we do this the same way as we obtained the posterior distribution in step 2.
		\begin{align}
		p(\boldsymbol{w} | \boldsymbol{t},\boldsymbol{x}) &= \mathcal{N}(\boldsymbol{w}|\boldsymbol{m}_N, \boldsymbol{S}_N)\\
		p(t | \boldsymbol{w}, \boldsymbol{t}, \boldsymbol{x}) &= \mathcal{N}(t | \boldsymbol{\phi}(x)^T\boldsymbol{w}, \beta^{-1}\\
		&\rightarrow\\
		p(t | x, \boldsymbol{t}, \boldsymbol{x}) &= \mathcal{N}(t | \boldsymbol{m}_N^T \boldsymbol{\phi}(x), \sigma_N^2(x))
		\end{align}
		With $\boldsymbol{m}_N$ and $\boldsymbol{S}_N$ defined as before, and
		\begin{align}
		\sigma_N^2(x) &= \frac{1}{\beta} + \boldsymbol{\phi}(x)^T\boldsymbol{S}_N\boldsymbol{\phi}(x)
		\end{align}
		\begin{align}
		p(t | x, \boldsymbol{t},\boldsymbol{x}, \alpha, \beta) &= \mathcal{N}(t | m(x), s^2(x))\\
		m(x) &= \boldsymbol{m}_N^T \boldsymbol{\phi}(x)\\
		&= \begin{pmatrix} -0.0445 \\ -0.2021 \end{pmatrix}^T \begin{pmatrix} 1 \\ x \end{pmatrix}\\
		s(x) &= \sigma_N^2\\
		&= \frac{1}{10} + \begin{pmatrix} 1 \\ x \end{pmatrix}^T \begin{pmatrix} 22 & 10 \\ 10 & 7.2 \end{pmatrix}^{-1} \begin{pmatrix} 1 \\ x \end{pmatrix}
		\end{align}
	\item See Figure 1.1 where the mean of the predictive Gaussian distribution and one standard deviation on both sides are plotted as a function over $x$ over the interval $[0,1]$, with the two observed data points. When we compare this plot with Figure 3.8b (Bishop, p.157), is our linear mean. This can be explained by the nature of our basis function $\phi_j$, which is two-dimensional in our case and Bishop uses a 9-dimensional function.
		\begin{figure}[h]
   			\centering
   			\includegraphics[width=0.95\textwidth]{exercise1plottanopink.png}\vspace{-0.4cm}
   			\caption{\vspace{-0.1cm} The mean of the predictive Gaussian distribution is represented by the green line, plotted against $x$. \vspace{-0.1cm}The two grey dots are the data points $\{x_1,t_1\}$ and $\{x_2,t_2\}$. The light green area indicates the standard deviation and is bound by the mean plus the standard deviationd and the mean minus the standard deviation.}\vspace{-0.2cm}
  		\end{figure}
	\item We sampled five functions $y(x, \boldsymbol{w})$ from the posterior distribution over $\boldsymbol{w}$ (see Exercise 1.1 step 2) for this data set and plotted them with the predictive distribution (see Figure 1.2). 
		\begin{figure}[h!]
   			\centering
   			\includegraphics[width=0.95\textwidth]{exercise1plotta.png}\vspace{-0.4cm}
   			\caption{\vspace{-0.0cm} The pink lines represent $y(x,\boldsymbol{w})$ where the weights $\boldsymbol{w}$ are sampled from the posterior distribution with mean $m_N$ and variance $S_N$.}
  		\end{figure}
\end{enumerate}

\vfill 

\section{Logistic regression}
\subsection{The IRLS algorithm}
\begin{enumerate}
	\item Because our function $f(\boldsymbol{x}_n)$ is one-dimensional, we can take the second derivate of $f(\boldsymbol{x}_n)$ as our Hessian function, and rewrite Eq. 2.1 as Eq. 2.2. 
		\begin{align}
		\boldsymbol{x}_{n+1} &= \boldsymbol{x}_n - \boldsymbol{H}^{-1} \nabla f(\boldsymbol{x}_n)\\
		&= \boldsymbol{x}_n - \frac{\frac{df(\boldsymbol{x}_n)}{dx_n} f(\boldsymbol{x}_n)}{\frac{d^2f(\boldsymbol{x}_n)}{d^2\boldsymbol{x}_n} f(\boldsymbol{x}_n)}\\
		f(\boldsymbol{x}_n) &= sin(\boldsymbol{x}_n)\\
		\boldsymbol{x}_{n+1} &= \boldsymbol{x}_n - \frac{cos(\boldsymbol{x}_n)}{-sin(\boldsymbol{x}_n)} = \boldsymbol{x}_n + \frac{cos(\boldsymbol{x}_n)}{sin(\boldsymbol{x}_n)}
		\end{align}
		We implemented the Newton-Raphson iterative method in the functions depicted in Listing 1:
		\begin{lstlisting}[label={list:first},caption=Python code for function \textit{hessian\_sin(x)} and \textit{run\_hessian\_sin(x, n\_iter=5)}.]
def hessian_sin(x):
    return x + (np.cos(x)/np.sin(x))

def run_hessian_sin(x, n_iter=5):
    print "x_0:", x
    
    for i in xrange(n_iter):
        x = hessian_sin(x)
        print 'x_'+str(i+1)+":",x
    		\end{lstlisting}
		For $x_0 = 1$ this resulted in:
		\begin{align*}
		x_1&: 1.64209261593\\
		x_2&: 1.57067527716\\
		x_3&: 1.5707963268\\
		x_4&: 1.57079632679\\
		x_5&: 1.57079632679
		\end{align*}
		The algorithm converges to around 1.5708 after 4 iterations.\\
		For $x_0 = -1$ this resulted in: 
		\begin{align*}
		x_1&: -1.64209261593\\
		x_2&: -1.57067527716\\
		x_3&: -1.5707963268\\
		x_4&: -1.57079632679\\
		x_5&: -1.57079632679
		\end{align*}
		This is the same, except the negative value. For $x_0 = 1$, the value is the maximum of $sin(x)$, whereas for $x_0 = -1$, the value is the minimum of $sin(x)$, this is to be expected. Since this method finds the minima and maxima of a function, this is to be expected. 
	\item See Listing 2 for the code to calculate the IRLS algorithm on the given data. \begin{lstlisting}[label={list:first},caption=Python code for function \textit{sigmoid(x)} and \textit{H(phi, y)} and the $main$.]
def sigmoid(x):
    return 1.0/(1.0+np.exp(-x))
    
def H(phi, y):
    R = np.diag(np.ndarray.flatten(y * (1-y)))
    return np.dot(phi.T,(np.dot(R,phi)))


if __name__ == '__main__':
    phi = np.array([[1,0.3],[1,0.44],[1,0.46],[1,0.6]])
    t = np.array([[1],[0],[1],[0]])
    w = np.array([[1],[1]])
    
    for i in range(10):
        y = sigmoid(np.dot(phi,w))
        h = H(phi, y)
        
        w = w - np.dot(np.linalg.inv(h) , (np.dot(phi.T,y-t)))
        print i+1, w
    		\end{lstlisting}
		After 6 iterations our algorithm converges to $\hat{\boldsymbol{w}} = \begin{pmatrix} 9.78228 \\ -21.73839 \end{pmatrix}$, which corresponds to $\hat{\boldsymbol{w}} \approx \begin{pmatrix} 9.8 \\ -21.7 \end{pmatrix}$ as mentioned in the assignment as the optimal solution.\\
		We will show that this corresponds to a decision boundary of $\phi = 0.45$. Since $p(\mathcal{C}_1 | \boldsymbol{\phi})$ = $p(\mathcal{C}_2 | \boldsymbol{\phi})$ = 0.5 and we can fill in $\hat{\boldsymbol{w}} = \begin{pmatrix} 9.8 \\ -21.7 \end{pmatrix}$, we can solve the following equation to find $\phi$:
		\begin{align}
		p(\mathcal{C}_1 | \boldsymbol{\phi}) &= y(\boldsymbol{\phi}) = \sigma(\boldsymbol{w}^T\boldsymbol{\phi})\\
		0.5 &= \frac{1}{1 + \exp(\boldsymbol{w}^T\boldsymbol{\phi})}\\
		&= \frac{1}{1 + \exp(\begin{pmatrix} 9.8 & -21.7 \end{pmatrix} \begin{pmatrix} 1 \\ \phi \end{pmatrix})}\\
		2 &= 1 + \exp(\begin{pmatrix} 9.8 & -21.7 \end{pmatrix} \begin{pmatrix} 1 \\ \phi \end{pmatrix})\\
		1 &= \exp(\begin{pmatrix} 9.8 & -21.7 \end{pmatrix} \begin{pmatrix} 1 \\ \phi \end{pmatrix})\\
		0 &= \begin{pmatrix} 9.8 & -21.7 \end{pmatrix} \begin{pmatrix} 1 \\ \phi \end{pmatrix}\\
		&= 9.8 \cdot 1 -21.7\phi\\
		21.7\phi &= 9.8\\
		\phi &= 0.45
		\end{align} 
\end{enumerate}

\subsection{Two-class classification using logistic regression}
\begin{enumerate}
	\item The data is plotted and colored according to their labels in Figure 2.1. We think logistic regression is suitable for this problem, since there is a clear separation between classes. The data is not linearly separable by their classes, and thus using the standard feature space with a dummy basis function will not work. 
		\begin{figure}[h!]
   			\centering
   			\includegraphics[width=0.85\textwidth]{exercise2plotta.png}\vspace{-0.4cm}
   			\caption{\vspace{-0.0cm} Two-class data.}
  		\end{figure}
	\item In the IRLS algorithm, we modified the initialization of $\boldsymbol{w}$ and $\boldsymbol{\phi}$, and changed the algorithm a bit to be able to handle the new data as input. The resulting class probabilities are all 0.5:
		\begin{align}
		p(\mathcal{C}_1 | \boldsymbol{\phi}) &= y(\boldsymbol{\phi}) = \sigma(\boldsymbol{w}^T\boldsymbol{\phi})\\
		&= \sigma \begin{pmatrix} 0\\0\\0 \end{pmatrix}^T \boldsymbol{\phi}) = \sigma(0) \\
		&= \frac{1}{1+\exp(0)} = \frac{1}{1+1} = 0.5
		\end{align}
	\item See Figure 2.2. The probabilities range from 0.48 to 0.51, and therefore the colors in the plot are scaled to range from the minimum and maximum of the class probabilities, otherwise (ranging from 0 to 1) there is no visible difference in color for the data points. The initial cross entropy error with the initial weights was 693.14718, computed using the function in Listing 3. The cross entropy error only improves very slightly, and is now 692.96936. This is to be expected, since the class probabilities are all around 0.5 and thus the dummy basis function does not help differentiate between classes.
		\begin{figure}[h!]
   			\centering
   			\includegraphics[width=0.85\textwidth]{exercise2plottadummy.png}\vspace{-0.4cm}
   			\caption{\vspace{-0.0cm} Data colored according to the class probabilities calculated using $\{x_1,x_2\}$ basis function and the dummy basis function.}
  		\end{figure}
		 \begin{lstlisting}[label={list:first},caption=Python code for function \textit{cross\_entropy\_error(labels, y)}.]
def cross_entropy_error(labels,y):
    E = 0
    for label,_y in zip(labels,y):
        E += label * np.log(_y) + (1-label)*np.log(1-_y)
        
    return -E
    		\end{lstlisting}
	\item We calculated $\phi_1$ and $\phi_2$ using the function shown in Listing 4, called with $\boldsymbol{\mu}_1 = (0,0)$ for $\phi_1$ and $\boldsymbol{\mu}_2 = (1,1)$ for $\phi_2$. The resulting plot of the data in the feature domain is shown in Figure 2.3. With these features, we think logistic regression can be a very good approach to classify the data, since the different classes are fairly linearly distinguishable in this domain.
		\begin{lstlisting}[label={list:first},caption=Python code for function \textit{gaussian\_bf(x, mu)}.]
def gaussian_bf(x, mu):
    sigma = 0.2 * np.identity(2)
    return multivariate_normal.pdf(x,mu,sigma)
		\end{lstlisting}
		\begin{figure}[h!]
   			\centering
   			\includegraphics[width=0.85\textwidth]{exercise2curvy.png}\vspace{-0.4cm}
   			\caption{\vspace{-0.0cm} Data shown in the feature domain.}
  		\end{figure}
	\item The cross entropy error improved to 346.50408, it became much smaller than before (692.96936). The error halfed more or less. By mapping the features to a different feature space (by using the two Gaussian basis functions as features) the data became much better linearly separable, although some values near the boundaries still have fairly uncertain probabilities of belonging to either class (see Figure 2.4).
		\begin{figure}[h!]
   			\centering
   			\includegraphics[width=0.85\textwidth]{exercise2galaxy.png}\vspace{-0.4cm}
   			\caption{\vspace{-0.0cm} Data colored according to the class probabilities calculated using the Gaussian basis functions and the dummy basis function.}
  		\end{figure}
\end{enumerate}

\end{document}
