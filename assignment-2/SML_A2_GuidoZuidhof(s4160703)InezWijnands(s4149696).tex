\documentclass[a4paper,10pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fourier}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{tgtermes}

\usepackage[
pdftitle={Statistical Machine Learning}, 
pdfauthor={Inez Wijnands \& Guido Zuidhof, Radboud University Nijmegen},
colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue,bookmarks=true,
bookmarksopenlevel=2]{hyperref}
\usepackage{amsmath,amssymb,amsthm,textcomp}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{tikz}

\usepackage{geometry}
\geometry{total={210mm,297mm},
left=25mm,right=25mm,%
bindingoffset=0mm, top=20mm,bottom=20mm}

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1 instead of 1)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1 i/o 1)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1 i/of 1)

\linespread{1.35}

\newcommand{\linia}{\rule{\linewidth}{0.5pt}}

% custom theorems if needed
\newtheoremstyle{mytheor}
    {1ex}{1ex}{\normalfont}{0pt}{\scshape}{.}{1ex}
    {{\thmname{#1 }}{\thmnumber{#2}}{\thmnote{ (#3)}}}

\theoremstyle{mytheor}
\newtheorem{defi}{Definition}

% my own titles
\makeatletter
\renewcommand{\maketitle}{
\begin{center}
\vspace{2ex}
{\huge \textsc{\@title}}
\vspace{1ex}
\\
\linia\\
\@author  \@date
\vspace{4ex}
\end{center}
}
\makeatother
%%%

% custom footers and headers
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{Assignment \textnumero{} 2}
\cfoot{}
\rfoot{Page \thepage\ /\ \pageref*{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
%

% code listing settings
\usepackage{listings}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    aboveskip={1.0\baselineskip},
    belowskip={1.0\baselineskip},
    columns=fixed,
    extendedchars=true,
    breaklines=true,
    tabsize=4,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=lines,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\color[rgb]{0.627,0.126,0.941},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color[rgb]{01,0,0},
    numbers=left,
    numberstyle=\scriptsize\ttfamily,
    stepnumber=1,
    numbersep=10pt,
    captionpos=t,
    escapeinside={\%*}{*)}
}

%%%----------%%%----------%%%----------%%%----------%%%

\begin{document}

\title{Statistical Machine Learning \\ Assignment 2}

\author{Inez Wijnands (s4149696) \& Guido Zuidhof (s4160703)\\ Radboud University Nijmegen\\}

\date{04/11/2015}

\maketitle

\noindent \textit{The entire code listing is in a separate file. The listings shown here are merely code snippets}.\vspace{-0.5cm}
\section{Sequential learning}
\subsection{Obtaining the prior}
\begin{enumerate}
	\item \begin{align}
			\boldsymbol{\tilde \Lambda}_{a,b} &= \boldsymbol{\tilde \Sigma}_{a,b}^{-1} \\
			&= \newcommand*{\temp}{\multicolumn{1}{c|}{}} 
				 \begin{pmatrix}
					60&50&\temp&-48&38\\ 
					50&50&\temp&-50&40\\ \cline{1-5} 
					-48&-50&\temp&52.4&-41.4 \\
					38&40&\temp&-41.4&33.4
				\end{pmatrix}
		\end{align}\\\\
		Using the precision matrix $\boldsymbol{\tilde \Lambda}$ we can use equations 2.69, 2.73 and 2.75 from Bishop to obtain the mean and covariance of the conditional distribution $p([x_1,x_2]^T|x_3 = x_4 = 0)$.\\
		\begin{align}
			\boldsymbol{\Sigma}_p &= \boldsymbol \Lambda_{aa}^{-1} \tag{Bishop 2.73}\\
			\boldsymbol \Lambda_{aa} &= \begin{pmatrix} 60 & 50 \\ 50 & 50 \end{pmatrix}\\
			\boldsymbol{\Sigma}_p &= \begin{pmatrix} 0.1 & -0.1 \\ -0.1 & 0.12 \end{pmatrix}
		\end{align}
		\begin{align}
			\boldsymbol{\mu}_p = \boldsymbol{\mu}_{a | b} = \boldsymbol{\mu}_a - \boldsymbol{\Lambda}_{aa}^{-1}\boldsymbol{\Lambda}_{ab}(\boldsymbol{x}_b - \boldsymbol{\mu}_b) \tag{Bishop 2.75}
		\end{align}
		We can fill in this equation, since $\boldsymbol {\tilde \mu}$ and $\boldsymbol x_b$ (the second partition of $\boldsymbol x$) are known.
		\begin{align}
			\boldsymbol{\mu}_p &=
			\begin{pmatrix} 1\\ 0 \end{pmatrix}-
			\begin{pmatrix} 60 & 50 \\ 50 & 50 \end{pmatrix}^{-1}
			\begin{pmatrix} -48 & 38\\ -50 & 40 \end{pmatrix}
			(\begin{pmatrix} 0\\ 0 \end{pmatrix}-
			\begin{pmatrix} 1\\ 2 \end{pmatrix}) \\
			&= \begin{pmatrix} 1\\ 0 \end{pmatrix}-
			\begin{pmatrix} 0.1 & -0.1 \\ -0.1 & 0.12 \end{pmatrix}
			\begin{pmatrix} -48 & 38\\ -50 & 40 \end{pmatrix}
			(\begin{pmatrix} 0\\ 0 \end{pmatrix}-
			\begin{pmatrix} 1\\ 2 \end{pmatrix}) \\
			&= \begin{pmatrix} 0.8\\ 0.8 \end{pmatrix}
		\end{align}
	\item Using the prior $\boldsymbol \mu_p$ and $\boldsymbol \Sigma_p$, we used the numpy-equivalent in Python for the MATLAB-function \verb|mvnrnd| to obtain the $\boldsymbol \mu_t$ we used for the remainder of this assignment:
		\begin{verbatim}
			np.random.multivariate_normal(mu_p, sigma_p, 1)
		\end{verbatim}
		This resulted in:
		\begin{equation}
			\boldsymbol\mu_t = \begin{pmatrix} 0.28584241 \\ 1.42626702\end{pmatrix}
		\end{equation}
	\item The probability density is highest at the mean (as illustrated in Figure 1.1). The density decreases quickly as both x and y change, but less so when $x$ XOR $y$ change. In $\boldsymbol \Sigma_p$, the values for the $x$ XOR $y$ are lower, so this is consistent with our density plot.
		\begin{figure}[h!]
   			\centering
   			\includegraphics{probdens.png}\vspace{-0.2cm}
   			\caption{\vspace{-0.2cm} The probability density of the distribution.}
  		\end{figure}
\end{enumerate}

\subsection{Generating the data}
\begin{enumerate}
	\item We used the following function to generate our data:
	\begin{verbatim}
		np.random.multivariate_normal(mu_t, sigma_t, 1000)
	\end{verbatim}
	\item \begin{align}\boldsymbol{\mu}_{ML} &= \frac{1}{N}  \sum_{n=1}^{N} \boldsymbol{x}_n \tag{Bishop 2.121}\\ 
	&= [\frac{1}{1000}  \sum_{n=1}^{1000} \boldsymbol{x}_n, \frac{1}{1000}  \sum_{n=1}^{1000} \boldsymbol{y}_n]\\
	&= \begin{pmatrix}
0.25383138\\
1.38260838
\end{pmatrix}
		\end{align}
		\begin{align}
		\boldsymbol{\Sigma}_{ML} &= \frac{1}{N} \sum_{n=1}^{N} (\boldsymbol{x}_n - \boldsymbol{\mu}_{ML})(\boldsymbol{x}_n - \boldsymbol{\mu}_{ML})^T \tag{Bishop 1.22}\\
		&= \begin{pmatrix}
\frac{1}{1000} \sum_{n=1}^{1000} (\boldsymbol{x}_n - \boldsymbol{\mu}_{ML}(1))(\boldsymbol{x}_n - \boldsymbol{\mu}_{ML}(1))^T & \frac{1}{1000} \sum_{n=1}^{1000} (\boldsymbol{x}_n - \boldsymbol{\mu}_{ML}(1))(\boldsymbol{y}_n - \boldsymbol{\mu}_{ML}(2))^T\\
\frac{1}{1000} \sum_{n=1}^{1000} (\boldsymbol{y}_n - \boldsymbol{\mu}_{ML}(2))(\boldsymbol{x}_n - \boldsymbol{\mu}_{ML}(1))^T & \frac{1}{1000} \sum_{n=1}^{1000} (\boldsymbol{y}_n - \boldsymbol{\mu}_{ML}(2))(\boldsymbol{y}_n - \boldsymbol{\mu}_{ML}(2))^T
\end{pmatrix}\\
		&= \begin{pmatrix}
1.90513804 & 0.72479489\\
0.72479489 & 3.81690496
\end{pmatrix}
		\end{align}
		This is calculated using the following code:
		\begin{verbatim}
		mu_ml = sum(data)/len(data)

sse = [0,0]
for point in data:
        	point = np.matrix(point)
        	sse += (point-mu_ml).T*(point-mu_ml)
sigma_ml =  sse/len(data)
    		\end{verbatim}
    		The differences with the 'true' values are:
    		\begin{align}
    		\boldsymbol \mu_t - \boldsymbol \mu_{ML} &= \begin{pmatrix} 0.28584241 \\ 1.42626702\end{pmatrix} - \begin{pmatrix} 0.25383138\\ 1.38260838 \end{pmatrix} = \begin{pmatrix} 0.03201103\\ 0.04365864 \end{pmatrix}\\
    		\boldsymbol \Sigma_t - \boldsymbol \Sigma_{ML} &= \begin{pmatrix} 2.0 & 0.8\\ 0.8 & 4.0 \end{pmatrix} - \begin{pmatrix} 1.90513804 & 0.72479489\\ 0.72479489 & 3.81690496 \end{pmatrix} = \begin{pmatrix} 0.09486196 & 0.07520511\\ 0.07520511 & 0.18309504 \end{pmatrix}
		\end{align}
\end{enumerate}

\subsection{Sequential learning algorithms}
\begin{enumerate}
	\item
	\item \begin{equation}p(\boldsymbol{x} | D_{n-1}) = \mathcal{N}(\boldsymbol{x} | \boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1}) \tag{Bishop 2.113}
		\end{equation}
where: $\boldsymbol{x} = \boldsymbol{\mu}, \boldsymbol{\mu} = \boldsymbol{\mu}_{(n-1)}, \boldsymbol{\Lambda}^{-1} = \boldsymbol{\Sigma}_{(n-1)}$
		\begin{equation}p(\boldsymbol{y} | \boldsymbol{x}) = \mathcal{N}(\boldsymbol{y} | \boldsymbol{Ax} + \boldsymbol{b}, \boldsymbol{L}^{-1}) \tag{Bishop 2.114}
		\end{equation}
where $\boldsymbol{y} = \boldsymbol{x}_n, \boldsymbol{A} = \boldsymbol{I}, \boldsymbol{x} = \boldsymbol{\mu}, \boldsymbol{b} = 0, \boldsymbol{L}^{-1} = \boldsymbol{\Sigma}^t$
		\begin{align} p(\boldsymbol{x} | \boldsymbol y) &= \mathcal{N}(\boldsymbol{x} | \boldsymbol{\Sigma} \{ \boldsymbol{A}^T \boldsymbol{L}(\boldsymbol{y} - \boldsymbol{b}) + \boldsymbol{\Lambda\mu}\}, \boldsymbol{\Sigma}) \tag{Bishop 2.116}\\
	 	\boldsymbol{\Sigma} &= (\boldsymbol{\Lambda} + \boldsymbol{A}^T\boldsymbol{LA})^{-1} \tag{Bishop 2.117}
	 	\end{align}
		Matching the variables we get the following equations:
		\begin{align}
			p(\boldsymbol{\mu} | \boldsymbol{x}_n) &= \mathcal{N}(\boldsymbol{\mu} | \boldsymbol S \{ \boldsymbol{I}^T \boldsymbol{\Sigma}_t^{-1}(\boldsymbol{x}_n - 0) + \boldsymbol{\Sigma}_{(n-1)}^{-1}\}, \boldsymbol S)\\
			&= \mathcal{N}(\boldsymbol{\mu} | \boldsymbol S \{ \boldsymbol{I}^T \boldsymbol{\Sigma}_t^{-1} \boldsymbol{x}_n + \boldsymbol{\Sigma}_{(n-1)}^{-1}\}, \boldsymbol S)\\
			 &= \mathcal{N}(\boldsymbol{\mu} | \boldsymbol S \{ \boldsymbol{\Sigma}_t^{-1} \boldsymbol{x}_n + \boldsymbol{\Sigma}_{(n-1)}^{-1}\}, \boldsymbol S)\\
			\boldsymbol{S} &= (\boldsymbol{\Sigma}_{(n-1)}^{-1} + \boldsymbol{I}^T \boldsymbol{\Sigma}_t^{-1} \boldsymbol{I})^{-1}\\
			&= (\boldsymbol{\Sigma}_{(n-1)}^{-1} + \boldsymbol{\Sigma}_t^{-1})^{-1}
		\end{align}
		$\boldsymbol \mu_n$ is the mean of the distribution $p(\boldsymbol \mu | \boldsymbol x_n)$, so the functions we use for our sequential learning algorithm are:
		\begin{align}
			\boldsymbol \Sigma_n &= \boldsymbol S\\
			\boldsymbol \mu_n &= \boldsymbol \Sigma_n \{ \boldsymbol \Sigma_t^{-1} \boldsymbol x_n + \boldsymbol{\Sigma}_{n-1}^{-1}\}, \boldsymbol \Sigma_n)
		\end{align}
	\item
	\item
\end{enumerate}

\section{The faulty lighthouse}
\subsection{Constructing the model}
\begin{enumerate}
	\item 
	\item 
	\item
	\item
\end{enumerate}

\subsection{Generate the lighthouse data}
\begin{enumerate}
	\item
	\item 
\end{enumerate}

\subsection{Find the lighthouse}
\begin{enumerate}
	\item
	\item 
	\item 
\end{enumerate}
\end{document}
